# ë°ì´í„° íŒŒì´í”„ë¼ì¸ ëª¨ë‹ˆí„°ë§ ì˜ˆì œ

Readable HOFë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì‹œê°„ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ëª¨ë‹ˆí„°ë§ ë° í’ˆì§ˆ ê²€ì‚¬ë¥¼ ìˆ˜í–‰í•˜ëŠ” ì™„ì „í•œ ì˜ˆì œì…ë‹ˆë‹¤.

## ì „ì²´ ì½”ë“œ

```python
"""
RFS Readable HOFë¥¼ í™œìš©í•œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ

ì´ ì˜ˆì œëŠ” ë‹¤ìŒ ê¸°ëŠ¥ì„ í¬í•¨í•©ë‹ˆë‹¤:
- ì‹¤ì‹œê°„ ë°ì´í„° ìŠ¤íŠ¸ë¦¼ ëª¨ë‹ˆí„°ë§
- ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬
- ì´ìƒ íŒ¨í„´ íƒì§€
- ìë™ ì•Œë¦¼ ì‹œìŠ¤í…œ
- ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
- ì¼ì¼ ë¦¬í¬íŠ¸ ìƒì„±
"""

import asyncio
import json
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Any, AsyncGenerator, Optional
from dataclasses import dataclass
from enum import Enum

from rfs.hof.readable import (
    apply_rules_to, scan_data_for, extract_from, validate_config,
    required, range_check, custom_check, 
    create_log_entry
)
from rfs.core.result import Result, Success, Failure


class DataQuality(Enum):
    """ë°ì´í„° í’ˆì§ˆ ë ˆë²¨"""
    EXCELLENT = "excellent"
    GOOD = "good" 
    ACCEPTABLE = "acceptable"
    POOR = "poor"
    CRITICAL = "critical"


@dataclass
class DataBatch:
    """ë°ì´í„° ë°°ì¹˜ ëª¨ë¸"""
    id: str
    timestamp: datetime
    source: str
    records: List[Dict[str, Any]]
    metadata: Dict[str, Any]
    
    def __len__(self) -> int:
        return len(self.records)


@dataclass 
class QualityMetrics:
    """í’ˆì§ˆ ë©”íŠ¸ë¦­"""
    completeness: float  # ì™„ì„±ë„ (0-1)
    consistency: float   # ì¼ê´€ì„± (0-1)
    accuracy: float      # ì •í™•ë„ (0-1)
    timeliness: float    # ì ì‹œì„± (0-1)
    
    @property
    def overall_score(self) -> float:
        """ì „ì²´ í’ˆì§ˆ ì ìˆ˜"""
        return (self.completeness + self.consistency + 
                self.accuracy + self.timeliness) / 4
    
    @property
    def quality_level(self) -> DataQuality:
        """í’ˆì§ˆ ë ˆë²¨ ë¶„ë¥˜"""
        score = self.overall_score
        if score >= 0.9:
            return DataQuality.EXCELLENT
        elif score >= 0.8:
            return DataQuality.GOOD
        elif score >= 0.7:
            return DataQuality.ACCEPTABLE
        elif score >= 0.5:
            return DataQuality.POOR
        else:
            return DataQuality.CRITICAL


class DataPipelineMonitor:
    """ë°ì´í„° íŒŒì´í”„ë¼ì¸ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.setup_logging()
        self.quality_rules = self._load_quality_rules()
        self.anomaly_patterns = self._load_anomaly_patterns()
        self.alert_thresholds = self._load_alert_thresholds()
        self.metrics_history = []
        self.alert_count = 0
    
    def setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
    
    async def monitor_data_stream(self, data_stream: AsyncGenerator[DataBatch, None]):
        """ë°ì´í„° ìŠ¤íŠ¸ë¦¼ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§"""
        self.logger.info("ğŸš€ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ëª¨ë‹ˆí„°ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        batch_count = 0
        async for batch in data_stream:
            batch_count += 1
            self.logger.info(f"ğŸ“¦ ë°°ì¹˜ #{batch_count} ì²˜ë¦¬ ì¤‘... (ID: {batch.id})")
            
            try:
                # 1. ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬
                quality_issues = await self._check_data_quality(batch)
                
                # 2. ì´ìƒ íŒ¨í„´ íƒì§€
                anomalies = await self._detect_anomalies(batch)
                
                # 3. ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                performance_metrics = await self._collect_performance_metrics(batch)
                
                # 4. ë°°ì¹˜ ì²˜ë¦¬ ìƒíƒœ í™•ì¸
                processing_status = await self._check_processing_status(batch)
                
                # 5. ì•Œë¦¼ ì²˜ë¦¬
                if quality_issues or anomalies:
                    await self._send_alerts({
                        "batch_id": batch.id,
                        "timestamp": batch.timestamp,
                        "quality_issues": quality_issues,
                        "anomalies": anomalies,
                        "performance_metrics": performance_metrics,
                        "processing_status": processing_status
                    })
                
                # 6. ë©”íŠ¸ë¦­ ì €ì¥
                await self._store_metrics(batch, {
                    "quality_issues": quality_issues,
                    "anomalies": anomalies,
                    "performance_metrics": performance_metrics,
                    "processing_status": processing_status
                })
                
                self.logger.info(f"âœ… ë°°ì¹˜ #{batch_count} ì²˜ë¦¬ ì™„ë£Œ")
                
            except Exception as e:
                self.logger.error(f"âŒ ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                await self._handle_processing_error(batch, e)
    
    async def _check_data_quality(self, batch: DataBatch) -> List[Dict]:
        """ë°°ì¹˜ ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬"""
        
        # Readable HOFë¡œ í’ˆì§ˆ ê·œì¹™ ì ìš©
        quality_violations = (apply_rules_to(batch.records)
                             .using(self.quality_rules)
                             .with_context({
                                 "batch_id": batch.id,
                                 "source": batch.source,
                                 "record_count": len(batch.records),
                                 "timestamp": batch.timestamp
                             })
                             .collect_violations()
                             .filter_by(lambda v: v.severity in ["high", "critical"]))
        
        # í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°
        metrics = self._calculate_quality_metrics(batch.records)
        
        # ì„ê³„ê°’ ê²€ì‚¬
        threshold_violations = []
        if metrics.overall_score < self.alert_thresholds["quality_score_min"]:
            threshold_violations.append({
                "type": "quality_threshold",
                "metric": "overall_score",
                "value": metrics.overall_score,
                "threshold": self.alert_thresholds["quality_score_min"],
                "severity": "high" if metrics.overall_score < 0.5 else "medium"
            })
        
        all_issues = quality_violations.collect() + threshold_violations
        
        if all_issues:
            self.logger.warning(f"âš ï¸ í’ˆì§ˆ ì´ìŠˆ {len(all_issues)}ê°œ ë°œê²¬ (ë°°ì¹˜: {batch.id})")
        
        return all_issues
    
    async def _detect_anomalies(self, batch: DataBatch) -> List[Dict]:
        """ì´ìƒ íŒ¨í„´ íƒì§€"""
        
        # Readable HOFë¡œ ì´ìƒ íŒ¨í„´ ìŠ¤ìº”
        anomalies = (scan_data_for(self.anomaly_patterns)
                    .in_data(batch.records)
                    .extract(lambda record, pattern_name: {
                        "batch_id": batch.id,
                        "record_id": record.get("id", "unknown"),
                        "pattern": pattern_name,
                        "anomaly_score": self._calculate_anomaly_score(record, pattern_name),
                        "timestamp": record.get("timestamp", batch.timestamp),
                        "details": self._get_anomaly_details(record, pattern_name),
                        "severity": self._get_anomaly_severity(pattern_name)
                    })
                    .filter_by(lambda item: item["anomaly_score"] > 0.7)
                    .sort_by(lambda item: item["anomaly_score"], reverse=True))
        
        detected_anomalies = anomalies.collect()
        
        if detected_anomalies:
            self.logger.warning(f"ğŸš¨ ì´ìƒ íŒ¨í„´ {len(detected_anomalies)}ê°œ íƒì§€ (ë°°ì¹˜: {batch.id})")
        
        return detected_anomalies
    
    async def _collect_performance_metrics(self, batch: DataBatch) -> Dict:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        
        processing_start = datetime.now()
        
        # ë°°ì¹˜ ì²˜ë¦¬ ì„±ëŠ¥ ì¸¡ì •
        metrics = (extract_from(batch.records)
                  .transform_to(lambda record: {
                      "processing_time": self._measure_record_processing_time(record),
                      "memory_usage": self._measure_memory_usage(),
                      "cpu_usage": self._measure_cpu_usage(),
                      "record_size": len(str(record)) if record else 0
                  })
                  .collect())
        
        processing_end = datetime.now()
        batch_processing_time = (processing_end - processing_start).total_seconds()
        
        # ì„±ëŠ¥ ìš”ì•½
        performance_summary = {
            "batch_processing_time": batch_processing_time,
            "records_per_second": len(batch.records) / batch_processing_time if batch_processing_time > 0 else 0,
            "average_record_processing_time": sum(m["processing_time"] for m in metrics) / len(metrics) if metrics else 0,
            "total_memory_usage": max(m["memory_usage"] for m in metrics) if metrics else 0,
            "average_cpu_usage": sum(m["cpu_usage"] for m in metrics) / len(metrics) if metrics else 0,
            "total_data_size": sum(m["record_size"] for m in metrics),
            "throughput_mb_per_sec": sum(m["record_size"] for m in metrics) / (1024 * 1024) / batch_processing_time if batch_processing_time > 0 else 0
        }
        
        return performance_summary
    
    async def _check_processing_status(self, batch: DataBatch) -> Dict:
        """ë°°ì¹˜ ì²˜ë¦¬ ìƒíƒœ í™•ì¸"""
        
        # ë°°ì¹˜ ë©”íƒ€ë°ì´í„° ê²€ì¦
        metadata_validation = validate_config(batch.metadata).against_rules([
            required("source_system", "ì†ŒìŠ¤ ì‹œìŠ¤í…œ ì •ë³´ í•„ìš”"),
            required("pipeline_stage", "íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ ì •ë³´ í•„ìš”"),
            custom_check("data_freshness", 
                        lambda x: (datetime.now() - datetime.fromisoformat(x)).total_seconds() < 3600,
                        "ë°ì´í„°ê°€ 1ì‹œê°„ ì´ìƒ ì˜¤ë˜ë¨"),
            range_check("expected_records", 1, 1000000, "ì˜ˆìƒ ë ˆì½”ë“œ ìˆ˜ ë²”ìœ„ ì˜¤ë¥˜")
        ])
        
        status = {
            "batch_id": batch.id,
            "processing_timestamp": datetime.now(),
            "metadata_valid": metadata_validation.is_success(),
            "record_count": len(batch.records),
            "expected_count": batch.metadata.get("expected_records", len(batch.records)),
            "completion_rate": len(batch.records) / batch.metadata.get("expected_records", len(batch.records)) if batch.metadata.get("expected_records") else 1.0,
            "processing_stage": batch.metadata.get("pipeline_stage", "unknown")
        }
        
        if not metadata_validation.is_success():
            status["metadata_errors"] = metadata_validation.unwrap_error()
        
        return status
    
    async def _send_alerts(self, alert_data: Dict):
        """ì•Œë¦¼ ë°œì†¡"""
        
        self.alert_count += 1
        
        # ì•Œë¦¼ ì‹¬ê°ë„ ê²°ì •
        severity = self._determine_alert_severity(alert_data)
        
        # ì•Œë¦¼ ë©”ì‹œì§€ ìƒì„±
        alert_message = self._generate_alert_message(alert_data, severity)
        
        # ì•Œë¦¼ ë°œì†¡ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì´ë©”ì¼, ìŠ¬ë™, SMS ë“±)
        self.logger.warning(f"ğŸš¨ Alert #{self.alert_count} [{severity.upper()}]: {alert_message}")
        
        # ì‹¬ê°í•œ ì•Œë¦¼ì˜ ê²½ìš° ì¦‰ì‹œ ì²˜ë¦¬
        if severity == "critical":
            await self._handle_critical_alert(alert_data)
    
    async def _store_metrics(self, batch: DataBatch, monitoring_results: Dict):
        """ë©”íŠ¸ë¦­ ì €ì¥"""
        
        metric_entry = {
            "timestamp": datetime.now(),
            "batch_id": batch.id,
            "source": batch.source,
            "record_count": len(batch.records),
            "quality_score": self._calculate_quality_metrics(batch.records).overall_score,
            "anomaly_count": len(monitoring_results["anomalies"]),
            "quality_issues_count": len(monitoring_results["quality_issues"]),
            "performance_metrics": monitoring_results["performance_metrics"],
            "processing_status": monitoring_results["processing_status"]
        }
        
        self.metrics_history.append(metric_entry)
        
        # ë©”íŠ¸ë¦­ íˆìŠ¤í† ë¦¬ í¬ê¸° ì œí•œ (ë©”ëª¨ë¦¬ ê´€ë¦¬)
        if len(self.metrics_history) > 1000:
            self.metrics_history = self.metrics_history[-800:]
    
    async def generate_daily_report(self, date: str) -> Dict:
        """ì¼ì¼ ëª¨ë‹ˆí„°ë§ ë¦¬í¬íŠ¸ ìƒì„±"""
        
        self.logger.info(f"ğŸ“Š {date} ì¼ì¼ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
        
        # í•´ë‹¹ ë‚ ì§œì˜ ë©”íŠ¸ë¦­ í•„í„°ë§
        daily_metrics = [
            m for m in self.metrics_history 
            if m["timestamp"].strftime("%Y-%m-%d") == date
        ]
        
        if not daily_metrics:
            return {"error": f"{date}ì— ëŒ€í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}
        
        # Readable HOFë¡œ ë¦¬í¬íŠ¸ ìƒì„±
        report = (extract_from(daily_metrics)
                 .group_by(lambda metric: metric["source"])
                 .transform_to(lambda group: {
                     "source": group["key"],
                     "total_batches": len(group["items"]),
                     "total_records": sum(item["record_count"] for item in group["items"]),
                     "average_quality_score": sum(item["quality_score"] for item in group["items"]) / len(group["items"]),
                     "total_anomalies": sum(item["anomaly_count"] for item in group["items"]),
                     "total_quality_issues": sum(item["quality_issues_count"] for item in group["items"]),
                     "average_processing_time": sum(
                         item["performance_metrics"]["batch_processing_time"] 
                         for item in group["items"]
                     ) / len(group["items"]),
                     "peak_records_per_second": max(
                         item["performance_metrics"]["records_per_second"] 
                         for item in group["items"]
                     ),
                     "data_quality_trend": self._calculate_quality_trend(group["items"])
                 })
                 .sort_by(lambda item: item["average_quality_score"], reverse=True))
        
        # ì „ì²´ ìš”ì•½ ê³„ì‚°
        summary = {
            "date": date,
            "total_batches": len(daily_metrics),
            "total_records": sum(m["record_count"] for m in daily_metrics),
            "average_quality_score": sum(m["quality_score"] for m in daily_metrics) / len(daily_metrics),
            "total_alerts": self.alert_count,
            "sources": report.collect(),
            "recommendations": self._generate_daily_recommendations(daily_metrics)
        }
        
        self.logger.info(f"âœ… {date} ì¼ì¼ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ")
        return summary
    
    # ========== í—¬í¼ ë©”ì„œë“œë“¤ ==========
    
    def _load_quality_rules(self) -> List:
        """ë°ì´í„° í’ˆì§ˆ ê·œì¹™ ë¡œë“œ"""
        return [
            # í•„ìˆ˜ í•„ë“œ ê²€ì‚¬
            lambda records: self._check_required_fields(records),
            
            # ë°ì´í„° íƒ€ì… ì¼ê´€ì„± ê²€ì‚¬
            lambda records: self._check_data_type_consistency(records),
            
            # ê°’ ë²”ìœ„ ê²€ì‚¬
            lambda records: self._check_value_ranges(records),
            
            # ì¤‘ë³µ ë°ì´í„° ê²€ì‚¬
            lambda records: self._check_duplicate_records(records),
            
            # ë°ì´í„° ì‹ ì„ ë„ ê²€ì‚¬
            lambda records: self._check_data_freshness(records),
        ]
    
    def _load_anomaly_patterns(self) -> Dict:
        """ì´ìƒ íŒ¨í„´ ë¡œë“œ"""
        return {
            "unexpected_spike": lambda record: self._detect_spike_anomaly(record),
            "missing_data": lambda record: self._detect_missing_data_anomaly(record),
            "format_inconsistency": lambda record: self._detect_format_anomaly(record),
            "unusual_timing": lambda record: self._detect_timing_anomaly(record),
            "data_drift": lambda record: self._detect_data_drift_anomaly(record),
            "outlier_values": lambda record: self._detect_outlier_anomaly(record),
        }
    
    def _load_alert_thresholds(self) -> Dict:
        """ì•Œë¦¼ ì„ê³„ê°’ ë¡œë“œ"""
        return {
            "quality_score_min": 0.7,
            "anomaly_score_max": 0.8,
            "processing_time_max": 30.0,  # seconds
            "memory_usage_max": 512,      # MB
            "error_rate_max": 0.05,       # 5%
        }
    
    def _calculate_quality_metrics(self, records: List[Dict]) -> QualityMetrics:
        """í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        if not records:
            return QualityMetrics(0, 0, 0, 0)
        
        # ì™„ì„±ë„ (í•„ìˆ˜ í•„ë“œ ë¹„ìœ¨)
        required_fields = ["id", "timestamp", "data"]
        completeness = sum(
            1 for record in records 
            if all(field in record and record[field] is not None for field in required_fields)
        ) / len(records)
        
        # ì¼ê´€ì„± (ë°ì´í„° íƒ€ì… ì¼ê´€ì„±)
        consistency = self._calculate_consistency_score(records)
        
        # ì •í™•ë„ (ê°’ ìœ íš¨ì„±)
        accuracy = self._calculate_accuracy_score(records)
        
        # ì ì‹œì„± (ë°ì´í„° ì‹ ì„ ë„)
        timeliness = self._calculate_timeliness_score(records)
        
        return QualityMetrics(completeness, consistency, accuracy, timeliness)
    
    def _calculate_anomaly_score(self, record: Dict, pattern_name: str) -> float:
        """ì´ìƒ ì ìˆ˜ ê³„ì‚°"""
        base_scores = {
            "unexpected_spike": 0.9,
            "missing_data": 0.8,
            "format_inconsistency": 0.7,
            "unusual_timing": 0.6,
            "data_drift": 0.8,
            "outlier_values": 0.7,
        }
        
        base_score = base_scores.get(pattern_name, 0.5)
        
        # ë ˆì½”ë“œë³„ ê°€ì¤‘ì¹˜ ì ìš©
        if record.get("critical_field"):
            base_score *= 1.2
        
        return min(base_score, 1.0)
    
    def _determine_alert_severity(self, alert_data: Dict) -> str:
        """ì•Œë¦¼ ì‹¬ê°ë„ ê²°ì •"""
        quality_issues = alert_data.get("quality_issues", [])
        anomalies = alert_data.get("anomalies", [])
        
        # Critical: ì‹¬ê°í•œ í’ˆì§ˆ ì´ìŠˆ ë˜ëŠ” ë†’ì€ ì´ìƒ ì ìˆ˜
        if any(issue.get("severity") == "critical" for issue in quality_issues):
            return "critical"
        
        if any(anomaly.get("anomaly_score", 0) > 0.9 for anomaly in anomalies):
            return "critical"
        
        # High: ë‹¤ìˆ˜ì˜ ì´ìŠˆ ë˜ëŠ” ì¤‘ìš”í•œ ì´ìŠˆ
        if len(quality_issues) + len(anomalies) > 10:
            return "high"
        
        if any(issue.get("severity") == "high" for issue in quality_issues):
            return "high"
        
        # Medium: ì¼ë°˜ì ì¸ ì´ìŠˆë“¤
        return "medium"
    
    def _generate_alert_message(self, alert_data: Dict, severity: str) -> str:
        """ì•Œë¦¼ ë©”ì‹œì§€ ìƒì„±"""
        batch_id = alert_data["batch_id"]
        quality_count = len(alert_data.get("quality_issues", []))
        anomaly_count = len(alert_data.get("anomalies", []))
        
        message = f"ë°°ì¹˜ {batch_id}ì—ì„œ "
        
        if quality_count > 0:
            message += f"í’ˆì§ˆ ì´ìŠˆ {quality_count}ê°œ"
        
        if anomaly_count > 0:
            if quality_count > 0:
                message += ", "
            message += f"ì´ìƒ íŒ¨í„´ {anomaly_count}ê°œ"
        
        message += " ë°œê²¬ë¨"
        
        return message


# ë°ì´í„° ìŠ¤íŠ¸ë¦¼ ì‹œë®¬ë ˆì´í„°
async def simulate_data_stream() -> AsyncGenerator[DataBatch, None]:
    """ë°ì´í„° ìŠ¤íŠ¸ë¦¼ ì‹œë®¬ë ˆì´ì…˜"""
    
    batch_id = 1
    sources = ["user_events", "transaction_logs", "system_metrics", "api_calls"]
    
    while True:
        # ëœë¤ ë°°ì¹˜ ìƒì„±
        import random
        
        source = random.choice(sources)
        record_count = random.randint(50, 500)
        
        records = []
        for i in range(record_count):
            record = {
                "id": f"{source}_{batch_id}_{i}",
                "timestamp": datetime.now().isoformat(),
                "data": {"value": random.randint(1, 1000), "status": "active"},
                "user_id": f"user_{random.randint(1, 10000)}",
                "session_id": f"session_{random.randint(1, 1000)}"
            }
            
            # ê°€ë” ë¬¸ì œê°€ ìˆëŠ” ë°ì´í„° ìƒì„±
            if random.random() < 0.1:  # 10% í™•ë¥ ë¡œ ë¬¸ì œ ë°ì´í„°
                if random.random() < 0.5:
                    del record["timestamp"]  # í•„ìˆ˜ í•„ë“œ ëˆ„ë½
                else:
                    record["data"]["value"] = -999  # ì´ìƒê°’
            
            records.append(record)
        
        batch = DataBatch(
            id=f"batch_{batch_id}",
            timestamp=datetime.now(),
            source=source,
            records=records,
            metadata={
                "source_system": source,
                "pipeline_stage": "ingestion",
                "expected_records": record_count,
                "data_freshness": datetime.now().isoformat()
            }
        )
        
        yield batch
        
        batch_id += 1
        await asyncio.sleep(2)  # 2ì´ˆë§ˆë‹¤ ìƒˆ ë°°ì¹˜


# ì‚¬ìš© ì˜ˆì œ
async def main():
    """ë°ì´í„° íŒŒì´í”„ë¼ì¸ ëª¨ë‹ˆí„°ë§ ì‹¤í–‰ ì˜ˆì œ"""
    
    # 1. ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    monitor = DataPipelineMonitor()
    
    # 2. ë°ì´í„° ìŠ¤íŠ¸ë¦¼ ìƒì„±
    data_stream = simulate_data_stream()
    
    # 3. ëª¨ë‹ˆí„°ë§ íƒœìŠ¤í¬ ì‹œì‘
    monitor_task = asyncio.create_task(
        monitor.monitor_data_stream(data_stream)
    )
    
    # 4. ì¼ì • ì‹œê°„ í›„ ë¦¬í¬íŠ¸ ìƒì„± (ë³„ë„ íƒœìŠ¤í¬)
    async def generate_periodic_reports():
        await asyncio.sleep(30)  # 30ì´ˆ í›„ ë¦¬í¬íŠ¸ ìƒì„±
        today = datetime.now().strftime("%Y-%m-%d")
        report = await monitor.generate_daily_report(today)
        
        print("\n" + "="*80)
        print("ğŸ“Š RFS ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì¼ì¼ ë¦¬í¬íŠ¸")
        print("="*80)
        print(json.dumps(report, indent=2, ensure_ascii=False, default=str))
    
    report_task = asyncio.create_task(generate_periodic_reports())
    
    # 5. ëª¨ë‹ˆí„°ë§ ì‹¤í–‰ (30ì´ˆê°„)
    try:
        await asyncio.wait_for(
            asyncio.gather(monitor_task, report_task),
            timeout=60.0
        )
    except asyncio.TimeoutError:
        print("\nâœ… ëª¨ë‹ˆí„°ë§ ì„¸ì…˜ ì™„ë£Œ")
        monitor_task.cancel()
        report_task.cancel()
        
        # ìµœì¢… ë¦¬í¬íŠ¸
        today = datetime.now().strftime("%Y-%m-%d")
        final_report = await monitor.generate_daily_report(today)
        print("\nğŸ“ˆ ìµœì¢… ìš”ì•½:")
        print(f"  â€¢ ì²˜ë¦¬ëœ ë°°ì¹˜: {final_report['total_batches']}ê°œ")
        print(f"  â€¢ ì´ ë ˆì½”ë“œ: {final_report['total_records']:,}ê°œ")
        print(f"  â€¢ í‰ê·  í’ˆì§ˆ ì ìˆ˜: {final_report['average_quality_score']:.3f}")
        print(f"  â€¢ ì´ ì•Œë¦¼: {final_report['total_alerts']}ê°œ")


if __name__ == "__main__":
    # ë¹„ë™ê¸° ì‹¤í–‰
    asyncio.run(main())
```

## ì‹¤í–‰ ê²°ê³¼ ì˜ˆì‹œ

```
ğŸš€ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ëª¨ë‹ˆí„°ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...
ğŸ“¦ ë°°ì¹˜ #1 ì²˜ë¦¬ ì¤‘... (ID: batch_1)
âœ… ë°°ì¹˜ #1 ì²˜ë¦¬ ì™„ë£Œ
ğŸ“¦ ë°°ì¹˜ #2 ì²˜ë¦¬ ì¤‘... (ID: batch_2)
âš ï¸ í’ˆì§ˆ ì´ìŠˆ 2ê°œ ë°œê²¬ (ë°°ì¹˜: batch_2)
ğŸš¨ Alert #1 [MEDIUM]: ë°°ì¹˜ batch_2ì—ì„œ í’ˆì§ˆ ì´ìŠˆ 2ê°œ ë°œê²¬ë¨
âœ… ë°°ì¹˜ #2 ì²˜ë¦¬ ì™„ë£Œ
ğŸ“¦ ë°°ì¹˜ #3 ì²˜ë¦¬ ì¤‘... (ID: batch_3)
ğŸš¨ ì´ìƒ íŒ¨í„´ 1ê°œ íƒì§€ (ë°°ì¹˜: batch_3)
ğŸš¨ Alert #2 [HIGH]: ë°°ì¹˜ batch_3ì—ì„œ ì´ìƒ íŒ¨í„´ 1ê°œ ë°œê²¬ë¨
âœ… ë°°ì¹˜ #3 ì²˜ë¦¬ ì™„ë£Œ

================================================================================
ğŸ“Š RFS ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì¼ì¼ ë¦¬í¬íŠ¸
================================================================================
{
  "date": "2024-01-15",
  "total_batches": 15,
  "total_records": 3420,
  "average_quality_score": 0.847,
  "total_alerts": 5,
  "sources": [
    {
      "source": "user_events",
      "total_batches": 4,
      "total_records": 950,
      "average_quality_score": 0.892,
      "total_anomalies": 1,
      "total_quality_issues": 2,
      "average_processing_time": 0.125,
      "peak_records_per_second": 1200.5,
      "data_quality_trend": "improving"
    },
    {
      "source": "transaction_logs", 
      "total_batches": 5,
      "total_records": 1150,
      "average_quality_score": 0.834,
      "total_anomalies": 3,
      "total_quality_issues": 4,
      "average_processing_time": 0.180,
      "peak_records_per_second": 980.2,
      "data_quality_trend": "stable"
    }
  ],
  "recommendations": [
    "ğŸ’¡ user_events ì†ŒìŠ¤ì˜ í’ˆì§ˆì´ ìš°ìˆ˜í•©ë‹ˆë‹¤. ì´ íŒ¨í„´ì„ ë‹¤ë¥¸ ì†ŒìŠ¤ì—ë„ ì ìš©í•˜ì„¸ìš”.",
    "âš ï¸ transaction_logsì—ì„œ ì´ìƒ íŒ¨í„´ì´ ìì£¼ ë°œê²¬ë©ë‹ˆë‹¤. ë°ì´í„° ì†ŒìŠ¤ë¥¼ ì ê²€í•˜ì„¸ìš”.",
    "ğŸš€ ì „ì²´ì ì¸ ì²˜ë¦¬ ì„±ëŠ¥ì´ ì–‘í˜¸í•©ë‹ˆë‹¤. í˜„ì¬ ì„¤ì •ì„ ìœ ì§€í•˜ì„¸ìš”."
  ]
}

ğŸ“ˆ ìµœì¢… ìš”ì•½:
  â€¢ ì²˜ë¦¬ëœ ë°°ì¹˜: 15ê°œ
  â€¢ ì´ ë ˆì½”ë“œ: 3,420ê°œ  
  â€¢ í‰ê·  í’ˆì§ˆ ì ìˆ˜: 0.847
  â€¢ ì´ ì•Œë¦¼: 5ê°œ

âœ… ëª¨ë‹ˆí„°ë§ ì„¸ì…˜ ì™„ë£Œ
```

## í•µì‹¬ ê¸°ëŠ¥

### 1. Readable HOF íŒ¨í„´ í™œìš©

- **apply_rules_to()**: ë°ì´í„° í’ˆì§ˆ ê·œì¹™ ì ìš©
- **scan_data_for()**: ì´ìƒ íŒ¨í„´ íƒì§€
- **extract_from()**: ë©”íŠ¸ë¦­ ì²˜ë¦¬ ë° ê·¸ë£¹í™”
- **validate_config()**: ë©”íƒ€ë°ì´í„° ê²€ì¦

### 2. ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§

```python
quality_violations = (apply_rules_to(batch.records)
                     .using(self.quality_rules)
                     .with_context({"batch_id": batch.id})
                     .collect_violations()
                     .filter_by(lambda v: v.severity in ["high", "critical"]))
```

### 3. ì¢…í•©ì  ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬

- ì™„ì„±ë„, ì¼ê´€ì„±, ì •í™•ë„, ì ì‹œì„± í‰ê°€
- ìë™ ì´ìƒ íƒì§€ ë° ì•Œë¦¼
- ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
- ì¼ì¼ ë¦¬í¬íŠ¸ ìë™ ìƒì„±

ì´ ì˜ˆì œëŠ” Readable HOFê°€ ë³µì¡í•œ ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ì–´ë–»ê²Œ í™œìš©ë  ìˆ˜ ìˆëŠ”ì§€ ë³´ì—¬ì£¼ë©°, ì„ ì–¸ì ì´ê³  ì½ê¸° ì‰¬ìš´ ì½”ë“œë¡œ ê°•ë ¥í•œ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤.