#!/usr/bin/env python3
"""
RFS Framework Adoption Checker

V2 ì½”ë“œë² ì´ìŠ¤ì—ì„œ RFS í”„ë ˆì„ì›Œí¬ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ìˆì§€ë§Œ 
ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ë¶€ë¶„ì„ ê²€ì‚¬í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

This script analyzes the V2 codebase to identify opportunities to adopt
RFS (React Functional Services) framework features.
"""

import ast
import os
import sys
from pathlib import Path
from typing import Dict, List, Set, Tuple, Optional
from dataclasses import dataclass, field
from collections import defaultdict
import json
import argparse
import math
from datetime import datetime, timedelta


@dataclass
class RFSOpportunity:
    """RFS í”„ë ˆì„ì›Œí¬ ì ìš© ê¸°íšŒ"""
    file_path: str
    line_number: int
    opportunity_type: str
    description: str
    suggestion: str
    severity: str  # 'high', 'medium', 'low'
    
    def to_dict(self) -> dict:
        return {
            'file': self.file_path,
            'line': self.line_number,
            'type': self.opportunity_type,
            'description': self.description,
            'suggestion': self.suggestion,
            'severity': self.severity
        }


@dataclass
class FileAnalysisResult:
    """íŒŒì¼ ë¶„ì„ ê²°ê³¼"""
    file_path: str
    opportunities: List[RFSOpportunity] = field(default_factory=list)
    rfs_score: float = 0.0  # 0-100 scale
    
    def calculate_score(self) -> None:
        """RFS ì ìš© ì ìˆ˜ ê³„ì‚°"""
        # ê¸°ë³¸ ì ìˆ˜ 100ì—ì„œ ê¸°íšŒë‹¹ ê°ì 
        base_score = 100
        for opp in self.opportunities:
            if opp.severity == 'high':
                base_score -= 10
            elif opp.severity == 'medium':
                base_score -= 5
            else:
                base_score -= 2
        self.rfs_score = max(0, base_score)


@dataclass
class CategoryScore:
    """ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜"""
    name: str
    current: int
    total: int
    percentage: float
    weight: float
    weighted_score: float


@dataclass
class OverallMetrics:
    """ì „ì²´ ë©”íŠ¸ë¦­"""
    overall_score: float
    grade: str
    maturity_level: int
    category_scores: List[CategoryScore]
    estimated_hours: float
    total_batches: int
    trend_direction: str = "stable"
    trend_percentage: float = 0.0


class RFSScoreCalculator:
    """RFS í”„ë ˆì„ì›Œí¬ ì¢…í•© ìŠ¤ì½”ì–´ ê³„ì‚°ê¸°"""
    
    # ì¹´í…Œê³ ë¦¬ë³„ ê°€ì¤‘ì¹˜ (ì´í•© 1.0)
    CATEGORY_WEIGHTS = {
        'stateless_service': 0.25,      # ê°€ì¥ ì¤‘ìš”
        'service_method_decorator': 0.20,
        'cache_decorator': 0.15,
        'performance_decorator': 0.15,
        'validation_decorator': 0.10,
        'logging_decorator': 0.05,
        'retry_decorator': 0.05,
        'railway_pattern': 0.03,
        'result_type': 0.02
    }
    
    # ì„±ìˆ™ë„ ë ˆë²¨ ì„ê³„ê°’
    MATURITY_THRESHOLDS = [
        (90, 5, "Expert"),
        (75, 4, "Advanced"),
        (60, 3, "Intermediate"),
        (40, 2, "Growing"),
        (0, 1, "Beginner")
    ]
    
    def __init__(self, results: List[FileAnalysisResult], statistics: Dict):
        self.results = results
        self.statistics = statistics
        self.baseline_file = "scripts/quality/rfs_baseline.json"
        
    def calculate_comprehensive_score(self) -> OverallMetrics:
        """ì¢…í•© ì ìˆ˜ ê³„ì‚°"""
        category_scores = self._calculate_category_scores()
        overall_score = self._calculate_weighted_score(category_scores)
        grade = self._calculate_grade(overall_score)
        maturity_level, maturity_name = self._calculate_maturity(overall_score)
        estimated_hours = self._estimate_completion_time()
        total_batches = math.ceil(self.statistics['total_opportunities'] / 10)
        
        # íŠ¸ë Œë“œ ë¶„ì„
        trend_direction, trend_percentage = self._analyze_trend(overall_score)
        
        return OverallMetrics(
            overall_score=overall_score,
            grade=grade,
            maturity_level=maturity_level,
            category_scores=category_scores,
            estimated_hours=estimated_hours,
            total_batches=total_batches,
            trend_direction=trend_direction,
            trend_percentage=trend_percentage
        )
    
    def _calculate_category_scores(self) -> List[CategoryScore]:
        """ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ê³„ì‚°"""
        category_scores = []
        
        for category, weight in self.CATEGORY_WEIGHTS.items():
            current = self.statistics.get(category, 0)
            total = self._estimate_total_possible(category)
            percentage = (1 - current / max(total, 1)) * 100  # ê¸°íšŒê°€ ì ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
            weighted_score = percentage * weight
            
            category_scores.append(CategoryScore(
                name=category,
                current=current,
                total=total,
                percentage=percentage,
                weight=weight,
                weighted_score=weighted_score
            ))
            
        return category_scores
    
    def _estimate_total_possible(self, category: str) -> int:
        """ê° ì¹´í…Œê³ ë¦¬ì˜ ì´ ê°€ëŠ¥í•œ ìˆ˜ ì¶”ì •"""
        # ì‹¤ì œ ì½”ë“œë² ì´ìŠ¤ ë¶„ì„ì„ í†µí•œ ì¶”ì •ê°’
        estimates = {
            'stateless_service': 20,      # ì„œë¹„ìŠ¤ í´ë˜ìŠ¤ ìˆ˜
            'service_method_decorator': 150,  # ê³µê°œ ë©”ì„œë“œ ìˆ˜
            'cache_decorator': 300,       # ìºì‹œ ê°€ëŠ¥í•œ ë©”ì„œë“œ ìˆ˜
            'performance_decorator': 200,  # ì„±ëŠ¥ ì¸¡ì • ëŒ€ìƒ ë©”ì„œë“œ
            'validation_decorator': 400,   # ìœ íš¨ì„± ê²€ì¦ í•„ìš” ë©”ì„œë“œ
            'logging_decorator': 250,     # ë¡œê¹… ëŒ€ìƒ ë©”ì„œë“œ
            'retry_decorator': 50,        # ì¬ì‹œë„ í•„ìš” ë©”ì„œë“œ
            'railway_pattern': 500,       # try-catch ë¸”ë¡ ìˆ˜
            'result_type': 600           # ì˜ˆì™¸ ë°œìƒ ì§€ì  ìˆ˜
        }
        return estimates.get(category, 100)
    
    def _calculate_weighted_score(self, category_scores: List[CategoryScore]) -> float:
        """ê°€ì¤‘ í‰ê·  ì ìˆ˜ ê³„ì‚°"""
        return sum(score.weighted_score for score in category_scores)
    
    def _calculate_grade(self, score: float) -> str:
        """ì ìˆ˜ë¥¼ ë“±ê¸‰ìœ¼ë¡œ ë³€í™˜"""
        if score >= 95:
            return "A+"
        elif score >= 90:
            return "A"
        elif score >= 85:
            return "A-"
        elif score >= 80:
            return "B+"
        elif score >= 75:
            return "B"
        elif score >= 70:
            return "B-"
        elif score >= 65:
            return "C+"
        elif score >= 60:
            return "C"
        elif score >= 55:
            return "C-"
        elif score >= 50:
            return "D+"
        elif score >= 45:
            return "D"
        else:
            return "F"
    
    def _calculate_maturity(self, score: float) -> Tuple[int, str]:
        """ì„±ìˆ™ë„ ë ˆë²¨ ê³„ì‚°"""
        for threshold, level, name in self.MATURITY_THRESHOLDS:
            if score >= threshold:
                return level, name
        return 1, "Beginner"
    
    def _estimate_completion_time(self) -> float:
        """ì™„ë£Œ ì˜ˆìƒ ì‹œê°„ ê³„ì‚° (ì‹œê°„ ë‹¨ìœ„)"""
        # ê¸°íšŒ ìœ í˜•ë³„ ì˜ˆìƒ ì†Œìš” ì‹œê°„ (ë¶„)
        time_estimates = {
            'stateless_service': 30,      # 30ë¶„
            'service_method_decorator': 5,
            'cache_decorator': 15,
            'performance_decorator': 10,
            'validation_decorator': 8,
            'logging_decorator': 5,
            'retry_decorator': 20,
            'railway_pattern': 15,
            'result_type': 10
        }
        
        total_minutes = 0
        for opportunity_type, count in self.statistics.items():
            if opportunity_type in time_estimates:
                total_minutes += count * time_estimates[opportunity_type]
                
        return total_minutes / 60  # ì‹œê°„ìœ¼ë¡œ ë³€í™˜
    
    def _analyze_trend(self, current_score: float) -> Tuple[str, float]:
        """íŠ¸ë Œë“œ ë¶„ì„"""
        try:
            if os.path.exists(self.baseline_file):
                with open(self.baseline_file, 'r', encoding='utf-8') as f:
                    baseline = json.load(f)
                    
                previous_score = baseline.get('overall_score', current_score)
                difference = current_score - previous_score
                
                if abs(difference) < 1:
                    return "stable", 0.0
                elif difference > 0:
                    return "improving", difference
                else:
                    return "declining", abs(difference)
        except:
            pass
            
        return "unknown", 0.0
    
    def save_baseline(self, metrics: OverallMetrics) -> None:
        """í˜„ì¬ ê²°ê³¼ë¥¼ ê¸°ì¤€ì„ ìœ¼ë¡œ ì €ì¥"""
        baseline_data = {
            'timestamp': datetime.now().isoformat(),
            'overall_score': metrics.overall_score,
            'grade': metrics.grade,
            'maturity_level': metrics.maturity_level,
            'total_opportunities': self.statistics['total_opportunities'],
            'category_scores': {
                score.name: {
                    'current': score.current,
                    'total': score.total,
                    'percentage': score.percentage
                }
                for score in metrics.category_scores
            }
        }
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(os.path.dirname(self.baseline_file), exist_ok=True)
        
        with open(self.baseline_file, 'w', encoding='utf-8') as f:
            json.dump(baseline_data, f, indent=2, ensure_ascii=False)


@dataclass
class ApplicationBatch:
    """ì ìš© ë°°ì¹˜"""
    batch_id: int
    opportunities: List[RFSOpportunity]
    priority: str  # 'high', 'medium', 'low'
    estimated_time: float  # ì‹œê°„ ë‹¨ìœ„
    complexity: str  # 'easy', 'medium', 'complex'
    roi_score: float  # Return on Investment ì ìˆ˜


class BatchApplicator:
    """ë°°ì¹˜ ë‹¨ìœ„ ì ìš©ê¸°"""
    
    # ROI ê³„ì‚° ê°€ì¤‘ì¹˜
    ROI_WEIGHTS = {
        'severity': {'high': 10, 'medium': 5, 'low': 2},
        'impact': {'high': 8, 'medium': 4, 'low': 1},
        'effort': {'easy': 8, 'medium': 4, 'complex': 1}  # ë‚®ì€ ë…¸ë ¥ì´ ë†’ì€ ì ìˆ˜
    }
    
    def __init__(self, opportunities: List[RFSOpportunity], batch_size: int = 10):
        self.opportunities = opportunities
        self.batch_size = batch_size
        self.batches: List[ApplicationBatch] = []
        
    def create_batches(self) -> List[ApplicationBatch]:
        """ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ë°°ì¹˜ ìƒì„±"""
        # ROI ì ìˆ˜ë¡œ ì •ë ¬
        sorted_opportunities = self._sort_by_roi(self.opportunities)
        
        # ë°°ì¹˜ë¡œ ë¶„í• 
        self.batches = []
        for i in range(0, len(sorted_opportunities), self.batch_size):
            batch_opportunities = sorted_opportunities[i:i + self.batch_size]
            
            batch = ApplicationBatch(
                batch_id=len(self.batches) + 1,
                opportunities=batch_opportunities,
                priority=self._determine_batch_priority(batch_opportunities),
                estimated_time=self._estimate_batch_time(batch_opportunities),
                complexity=self._determine_batch_complexity(batch_opportunities),
                roi_score=self._calculate_batch_roi(batch_opportunities)
            )
            
            self.batches.append(batch)
            
        return self.batches
    
    def _sort_by_roi(self, opportunities: List[RFSOpportunity]) -> List[RFSOpportunity]:
        """ROI ì ìˆ˜ ê¸°ë°˜ ì •ë ¬"""
        def calculate_roi(opp: RFSOpportunity) -> float:
            severity_score = self.ROI_WEIGHTS['severity'][opp.severity]
            impact_score = self._calculate_impact_score(opp)
            effort_score = self._calculate_effort_score(opp)
            
            return severity_score * 0.4 + impact_score * 0.4 + effort_score * 0.2
        
        return sorted(opportunities, key=calculate_roi, reverse=True)
    
    def _calculate_impact_score(self, opp: RFSOpportunity) -> float:
        """ì˜í–¥ë„ ì ìˆ˜ ê³„ì‚°"""
        # ê¸°íšŒ ìœ í˜•ë³„ ì˜í–¥ë„
        impact_map = {
            'stateless_service': 'high',
            'service_method_decorator': 'medium',
            'cache_decorator': 'high',
            'performance_decorator': 'high',
            'validation_decorator': 'medium',
            'logging_decorator': 'low',
            'retry_decorator': 'medium',
            'railway_pattern': 'low',
            'result_type': 'low'
        }
        
        impact_level = impact_map.get(opp.opportunity_type, 'medium')
        return self.ROI_WEIGHTS['impact'][impact_level]
    
    def _calculate_effort_score(self, opp: RFSOpportunity) -> float:
        """ë…¸ë ¥ë„ ì ìˆ˜ ê³„ì‚°"""
        # ê¸°íšŒ ìœ í˜•ë³„ êµ¬í˜„ ë³µì¡ë„
        effort_map = {
            'stateless_service': 'medium',
            'service_method_decorator': 'easy',
            'cache_decorator': 'medium',
            'performance_decorator': 'easy',
            'validation_decorator': 'easy',
            'logging_decorator': 'easy',
            'retry_decorator': 'medium',
            'railway_pattern': 'complex',
            'result_type': 'complex'
        }
        
        effort_level = effort_map.get(opp.opportunity_type, 'medium')
        return self.ROI_WEIGHTS['effort'][effort_level]
    
    def _determine_batch_priority(self, opportunities: List[RFSOpportunity]) -> str:
        """ë°°ì¹˜ ìš°ì„ ìˆœìœ„ ê²°ì •"""
        high_count = sum(1 for opp in opportunities if opp.severity == 'high')
        medium_count = sum(1 for opp in opportunities if opp.severity == 'medium')
        
        if high_count > 0:
            return 'high'
        elif medium_count >= len(opportunities) * 0.7:  # 70% ì´ìƒì´ medium
            return 'medium'
        else:
            return 'low'
    
    def _estimate_batch_time(self, opportunities: List[RFSOpportunity]) -> float:
        """ë°°ì¹˜ ì˜ˆìƒ ì†Œìš” ì‹œê°„ ê³„ì‚°"""
        time_estimates = {
            'stateless_service': 0.5,      # 30ë¶„
            'service_method_decorator': 0.08,  # 5ë¶„
            'cache_decorator': 0.25,       # 15ë¶„
            'performance_decorator': 0.17,  # 10ë¶„
            'validation_decorator': 0.13,   # 8ë¶„
            'logging_decorator': 0.08,     # 5ë¶„
            'retry_decorator': 0.33,       # 20ë¶„
            'railway_pattern': 0.25,       # 15ë¶„
            'result_type': 0.17           # 10ë¶„
        }
        
        total_time = 0
        for opp in opportunities:
            total_time += time_estimates.get(opp.opportunity_type, 0.17)
            
        return total_time
    
    def _determine_batch_complexity(self, opportunities: List[RFSOpportunity]) -> str:
        """ë°°ì¹˜ ë³µì¡ë„ ê²°ì •"""
        complex_types = ['railway_pattern', 'result_type', 'stateless_service']
        complex_count = sum(1 for opp in opportunities 
                          if opp.opportunity_type in complex_types)
        
        if complex_count >= len(opportunities) * 0.5:  # 50% ì´ìƒ ë³µì¡í•¨
            return 'complex'
        elif complex_count > 0:
            return 'medium'
        else:
            return 'easy'
    
    def _calculate_batch_roi(self, opportunities: List[RFSOpportunity]) -> float:
        """ë°°ì¹˜ ROI ì ìˆ˜ ê³„ì‚°"""
        if not opportunities:
            return 0.0
            
        total_roi = 0
        for opp in opportunities:
            severity_score = self.ROI_WEIGHTS['severity'][opp.severity]
            impact_score = self._calculate_impact_score(opp)
            effort_score = self._calculate_effort_score(opp)
            
            roi = severity_score * 0.4 + impact_score * 0.4 + effort_score * 0.2
            total_roi += roi
            
        return total_roi / len(opportunities)
    
    def get_batch_by_id(self, batch_id: int) -> Optional[ApplicationBatch]:
        """ë°°ì¹˜ IDë¡œ ë°°ì¹˜ ì¡°íšŒ"""
        for batch in self.batches:
            if batch.batch_id == batch_id:
                return batch
        return None
    
    def generate_batch_summary(self) -> str:
        """ë°°ì¹˜ ìš”ì•½ ìƒì„±"""
        if not self.batches:
            return "No batches created yet."
            
        lines = []
        lines.append(f"ğŸ“¦ RFS Adoption Batches ({len(self.opportunities)} opportunities â†’ {len(self.batches)} batches)")
        lines.append("")
        
        for batch in self.batches[:5]:  # ìƒìœ„ 5ê°œ ë°°ì¹˜ë§Œ í‘œì‹œ
            priority_emoji = {"high": "ğŸ”´", "medium": "ğŸŸ¡", "low": "ğŸŸ¢"}[batch.priority]
            complexity_emoji = {"complex": "ğŸ”´", "medium": "ğŸŸ¡", "easy": "ğŸŸ¢"}[batch.complexity]
            
            lines.append(f"Batch #{batch.batch_id} ({priority_emoji} {batch.priority.upper()}, "
                        f"{complexity_emoji} {batch.complexity}, "
                        f"Est. {batch.estimated_time:.1f}h)")
            lines.append("â”€" * 60)
            
            for i, opp in enumerate(batch.opportunities[:3], 1):  # ìƒìœ„ 3ê°œë§Œ í‘œì‹œ
                lines.append(f"{i}. {opp.file_path}:{opp.line_number}")
                lines.append(f"   âœ {opp.description} ({opp.severity.upper()})")
                
            if len(batch.opportunities) > 3:
                lines.append(f"   ... and {len(batch.opportunities) - 3} more items")
                
            lines.append("")
            
        if len(self.batches) > 5:
            lines.append(f"... and {len(self.batches) - 5} more batches")
            lines.append("")
            
        lines.append("ğŸ’¡ Run with --apply-batch [N] to apply a specific batch")
        lines.append("ğŸ’¡ Run with --show-batches to see all batches")
        
        return "\n".join(lines)


class RFSAdoptionChecker:
    """RFS í”„ë ˆì„ì›Œí¬ ì ìš© ê²€ì‚¬ê¸°"""
    
    # RFS ë°ì½”ë ˆì´í„° ëª©ë¡
    RFS_DECORATORS = {
        'service_method': 'Service method marking and metrics',
        'transactional': 'Transaction management',
        'cache_result': 'Result caching',
        'log_execution': 'Execution logging',
        'measure_performance': 'Performance measurement',
        'retry': 'Automatic retry on failure',
        'validate_args': 'Argument validation'
    }
    
    # ì—ëŸ¬ ì²˜ë¦¬ íŒ¨í„´
    ERROR_PATTERNS = {
        'try_except': 'Could use Railway Pattern',
        'raise_exception': 'Could return Result type',
        'none_check': 'Could use Option/Maybe pattern'
    }
    
    def __init__(self, target_dir: str = "app/services/v2"):
        self.target_dir = Path(target_dir)
        self.results: List[FileAnalysisResult] = []
        self.statistics = defaultdict(int)
        
    def analyze(self) -> None:
        """V2 ë””ë ‰í† ë¦¬ ë¶„ì„"""
        print(f"ğŸ” Analyzing RFS adoption in {self.target_dir}...")
        
        py_files = list(self.target_dir.rglob("*.py"))
        for py_file in py_files:
            if self._should_skip_file(py_file):
                continue
                
            result = self._analyze_file(py_file)
            if result.opportunities:
                self.results.append(result)
                
        self._calculate_statistics()
        
    def _should_skip_file(self, file_path: Path) -> bool:
        """ìŠ¤í‚µí•  íŒŒì¼ í™•ì¸"""
        skip_patterns = [
            '__pycache__',
            'test_',
            '_test.py',
            'mock',
            '__init__.py',
            'config/',
            'di/',  # DI ê´€ë ¨ íŒŒì¼ì€ ì´ë¯¸ í”„ë ˆì„ì›Œí¬ ì ìš©ë¨
        ]
        
        str_path = str(file_path)
        return any(pattern in str_path for pattern in skip_patterns)
        
    def _analyze_file(self, file_path: Path) -> FileAnalysisResult:
        """íŒŒì¼ ë¶„ì„"""
        # ìƒëŒ€ ê²½ë¡œë¡œ ë³€í™˜
        try:
            rel_path = file_path.relative_to(Path.cwd())
            result = FileAnalysisResult(file_path=str(rel_path))
        except ValueError:
            result = FileAnalysisResult(file_path=str(file_path))
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                source = f.read()
                tree = ast.parse(source)
                
            # ê° ê²€ì‚¬ ìˆ˜í–‰
            self._check_stateless_service(tree, source, result)
            self._check_decorator_opportunities(tree, source, result)
            self._check_error_handling(tree, source, result)
            self._check_manual_caching(tree, source, result)
            self._check_manual_retry(tree, source, result)
            self._check_manual_validation(tree, source, result)
            self._check_logging_opportunities(tree, source, result)
            self._check_performance_monitoring(tree, source, result)
            
            result.calculate_score()
            
        except Exception as e:
            print(f"âš ï¸  Error analyzing {file_path}: {e}")
            
        return result
        
    def _check_stateless_service(self, tree: ast.AST, source: str, result: FileAnalysisResult) -> None:
        """StatelessService ìƒì† ê¸°íšŒ ê²€ì‚¬"""
        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef):
                # Service, Manager, Processor ë“±ì˜ ì´ë¦„ì„ ê°€ì§„ í´ë˜ìŠ¤
                if any(pattern in node.name for pattern in ['Service', 'Manager', 'Processor', 'Handler']):
                    # StatelessServiceë¥¼ ìƒì†í•˜ì§€ ì•ŠëŠ” ê²½ìš°
                    if not self._inherits_stateless_service(node):
                        result.opportunities.append(RFSOpportunity(
                            file_path=result.file_path,
                            line_number=node.lineno,
                            opportunity_type='stateless_service',
                            description=f"Class '{node.name}' could extend StatelessService",
                            suggestion=f"Make '{node.name}' extend StatelessService for automatic service management",
                            severity='high'
                        ))
                        
    def _inherits_stateless_service(self, node: ast.ClassDef) -> bool:
        """StatelessService ìƒì† ì—¬ë¶€ í™•ì¸"""
        for base in node.bases:
            if isinstance(base, ast.Name) and base.id == 'StatelessService':
                return True
            elif isinstance(base, ast.Attribute) and base.attr == 'StatelessService':
                return True
        return False
        
    def _check_decorator_opportunities(self, tree: ast.AST, source: str, result: FileAnalysisResult) -> None:
        """ë°ì½”ë ˆì´í„° ì‚¬ìš© ê¸°íšŒ ê²€ì‚¬"""
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                # ê³µê°œ ë©”ì„œë“œì¸ ê²½ìš°
                if not node.name.startswith('_'):
                    decorators = [d.id if isinstance(d, ast.Name) else 
                                 (d.attr if isinstance(d, ast.Attribute) else None) 
                                 for d in node.decorator_list]
                    
                    # service_method ë°ì½”ë ˆì´í„° ë¯¸ì‚¬ìš©
                    if 'service_method' not in decorators:
                        # í´ë˜ìŠ¤ ë©”ì„œë“œì´ê³  ì„œë¹„ìŠ¤ ê´€ë ¨ ë©”ì„œë“œì¸ ê²½ìš°
                        if self._is_service_method(node):
                            result.opportunities.append(RFSOpportunity(
                                file_path=result.file_path,
                                line_number=node.lineno,
                                opportunity_type='service_method_decorator',
                                description=f"Method '{node.name}' could use @service_method decorator",
                                suggestion="Add @service_method for automatic metrics and error handling",
                                severity='medium'
                            ))
                            
    def _is_service_method(self, node: ast.FunctionDef) -> bool:
        """ì„œë¹„ìŠ¤ ë©”ì„œë“œì¸ì§€ í™•ì¸"""
        # self íŒŒë¼ë¯¸í„°ê°€ ìˆëŠ” ì¸ìŠ¤í„´ìŠ¤ ë©”ì„œë“œ
        if node.args.args and node.args.args[0].arg == 'self':
            # ì¼ë°˜ì ì¸ ì„œë¹„ìŠ¤ ë©”ì„œë“œ íŒ¨í„´
            service_patterns = [
                'process', 'handle', 'execute', 'perform',
                'analyze', 'enhance', 'translate', 'extract',
                'validate', 'transform', 'convert', 'generate'
            ]
            return any(pattern in node.name.lower() for pattern in service_patterns)
        return False
        
    def _check_error_handling(self, tree: ast.AST, source: str, result: FileAnalysisResult) -> None:
        """ì—ëŸ¬ ì²˜ë¦¬ íŒ¨í„´ ê²€ì‚¬"""
        for node in ast.walk(tree):
            # try-except ë¸”ë¡
            if isinstance(node, ast.Try):
                # Railway Pattern ëŒ€ì‹  ì‚¬ìš© ê°€ëŠ¥
                result.opportunities.append(RFSOpportunity(
                    file_path=result.file_path,
                    line_number=node.lineno,
                    opportunity_type='railway_pattern',
                    description="Try-except block could use Railway Pattern",
                    suggestion="Consider using Result[T, E] type with success/failure for functional error handling",
                    severity='low'
                ))
                
            # raise ë¬¸
            elif isinstance(node, ast.Raise):
                result.opportunities.append(RFSOpportunity(
                    file_path=result.file_path,
                    line_number=node.lineno,
                    opportunity_type='result_type',
                    description="Exception raising could use Result type",
                    suggestion="Return failure(error) instead of raising exceptions",
                    severity='low'
                ))
                
    def _check_manual_caching(self, tree: ast.AST, source: str, result: FileAnalysisResult) -> None:
        """ìˆ˜ë™ ìºì‹± íŒ¨í„´ ê²€ì‚¬"""
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                # í•¨ìˆ˜ ë‚´ì—ì„œ ìºì‹œ ê´€ë ¨ ì½”ë“œ ì°¾ê¸°
                for inner_node in ast.walk(node):
                    if isinstance(inner_node, ast.Name):
                        if 'cache' in inner_node.id.lower():
                            # @cache_result ë°ì½”ë ˆì´í„° ì‚¬ìš© ê°€ëŠ¥
                            decorators = [d.id if isinstance(d, ast.Name) else 
                                        (d.attr if isinstance(d, ast.Attribute) else None) 
                                        for d in node.decorator_list]
                            
                            if 'cache_result' not in decorators:
                                result.opportunities.append(RFSOpportunity(
                                    file_path=result.file_path,
                                    line_number=node.lineno,
                                    opportunity_type='cache_decorator',
                                    description=f"Method '{node.name}' has manual caching",
                                    suggestion="Use @cache_result decorator for automatic caching",
                                    severity='medium'
                                ))
                                break
                                
    def _check_manual_retry(self, tree: ast.AST, source: str, result: FileAnalysisResult) -> None:
        """ìˆ˜ë™ ì¬ì‹œë„ íŒ¨í„´ ê²€ì‚¬"""
        for node in ast.walk(tree):
            if isinstance(node, ast.For) or isinstance(node, ast.While):
                # ì¬ì‹œë„ íŒ¨í„´ ì°¾ê¸°
                for inner_node in ast.walk(node):
                    if isinstance(inner_node, ast.Name):
                        if any(word in inner_node.id.lower() for word in ['retry', 'attempt', 'tries']):
                            result.opportunities.append(RFSOpportunity(
                                file_path=result.file_path,
                                line_number=node.lineno,
                                opportunity_type='retry_decorator',
                                description="Manual retry logic detected",
                                suggestion="Use @retry decorator for automatic retry handling",
                                severity='medium'
                            ))
                            break
                            
    def _check_manual_validation(self, tree: ast.AST, source: str, result: FileAnalysisResult) -> None:
        """ìˆ˜ë™ ìœ íš¨ì„± ê²€ì¦ íŒ¨í„´ ê²€ì‚¬"""
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                # í•¨ìˆ˜ ì‹œì‘ ë¶€ë¶„ì—ì„œ ìœ íš¨ì„± ê²€ì¦ ì½”ë“œ ì°¾ê¸°
                validation_found = False
                for i, stmt in enumerate(node.body[:5]):  # ì²˜ìŒ 5ê°œ ë¬¸ì¥ë§Œ í™•ì¸
                    if isinstance(stmt, ast.If):
                        # íŒŒë¼ë¯¸í„° ê²€ì¦ íŒ¨í„´
                        for inner_node in ast.walk(stmt):
                            if isinstance(inner_node, ast.Compare):
                                validation_found = True
                                break
                                
                if validation_found:
                    decorators = [d.id if isinstance(d, ast.Name) else 
                                (d.attr if isinstance(d, ast.Attribute) else None) 
                                for d in node.decorator_list]
                    
                    if 'validate_args' not in decorators:
                        result.opportunities.append(RFSOpportunity(
                            file_path=result.file_path,
                            line_number=node.lineno,
                            opportunity_type='validation_decorator',
                            description=f"Method '{node.name}' has manual validation",
                            suggestion="Use @validate_args decorator for argument validation",
                            severity='low'
                        ))
                        
    def _check_logging_opportunities(self, tree: ast.AST, source: str, result: FileAnalysisResult) -> None:
        """ë¡œê¹… ê¸°íšŒ ê²€ì‚¬"""
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                # ë¡œê¹… ì½”ë“œ ì°¾ê¸°
                has_logging = False
                for inner_node in ast.walk(node):
                    if isinstance(inner_node, ast.Attribute):
                        if inner_node.attr in ['info', 'debug', 'warning', 'error']:
                            has_logging = True
                            break
                            
                if has_logging:
                    decorators = [d.id if isinstance(d, ast.Name) else 
                                (d.attr if isinstance(d, ast.Attribute) else None) 
                                for d in node.decorator_list]
                    
                    if 'log_execution' not in decorators:
                        result.opportunities.append(RFSOpportunity(
                            file_path=result.file_path,
                            line_number=node.lineno,
                            opportunity_type='logging_decorator',
                            description=f"Method '{node.name}' has manual logging",
                            suggestion="Use @log_execution decorator for automatic logging",
                            severity='low'
                        ))
                        
    def _check_performance_monitoring(self, tree: ast.AST, source: str, result: FileAnalysisResult) -> None:
        """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ê¸°íšŒ ê²€ì‚¬"""
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                # time ê´€ë ¨ ì½”ë“œ ì°¾ê¸°
                has_timing = False
                for inner_node in ast.walk(node):
                    if isinstance(inner_node, ast.Name):
                        if 'time' in inner_node.id.lower():
                            has_timing = True
                            break
                            
                if has_timing:
                    decorators = [d.id if isinstance(d, ast.Name) else 
                                (d.attr if isinstance(d, ast.Attribute) else None) 
                                for d in node.decorator_list]
                    
                    if 'measure_performance' not in decorators:
                        result.opportunities.append(RFSOpportunity(
                            file_path=result.file_path,
                            line_number=node.lineno,
                            opportunity_type='performance_decorator',
                            description=f"Method '{node.name}' has manual performance monitoring",
                            suggestion="Use @measure_performance decorator for automatic metrics",
                            severity='medium'
                        ))
                        
    def _calculate_statistics(self) -> None:
        """í†µê³„ ê³„ì‚°"""
        for result in self.results:
            for opp in result.opportunities:
                self.statistics[opp.opportunity_type] += 1
                self.statistics[f"severity_{opp.severity}"] += 1
                
        self.statistics['total_files'] = len(self.results)
        self.statistics['total_opportunities'] = sum(
            len(r.opportunities) for r in self.results
        )
        
        # í‰ê·  ì ìˆ˜ ê³„ì‚°
        if self.results:
            self.statistics['average_score'] = sum(
                r.rfs_score for r in self.results
            ) / len(self.results)
        else:
            self.statistics['average_score'] = 100.0
    
    def _get_all_opportunities(self) -> List[RFSOpportunity]:
        """ëª¨ë“  ê¸°íšŒ ëª©ë¡ ë°˜í™˜"""
        all_opportunities = []
        for result in self.results:
            all_opportunities.extend(result.opportunities)
        return all_opportunities
            
    def generate_report(self, format: str = 'text', dashboard: bool = False, 
                       show_batches: bool = False) -> str:
        """ë³´ê³ ì„œ ìƒì„±"""
        if dashboard:
            return self._generate_dashboard_report()
        elif show_batches:
            return self._generate_batch_report()
        elif format == 'json':
            return self._generate_json_report()
        elif format == 'markdown':
            return self._generate_markdown_report()
        else:
            return self._generate_text_report()
    
    def _generate_dashboard_report(self) -> str:
        """ëŒ€ì‹œë³´ë“œ ìŠ¤íƒ€ì¼ ë³´ê³ ì„œ ìƒì„±"""
        # ì¢…í•© ë©”íŠ¸ë¦­ ê³„ì‚°
        calculator = RFSScoreCalculator(self.results, self.statistics)
        metrics = calculator.calculate_comprehensive_score()
        
        lines = []
        lines.append("â•" * 80)
        lines.append("            RFS Framework Adoption Dashboard")
        lines.append("â•" * 80)
        lines.append("")
        
        # ì „ì²´ ê±´ê°•ë„ ì ìˆ˜
        trend_emoji = {
            "improving": "ğŸ“ˆ",
            "declining": "ğŸ“‰", 
            "stable": "ğŸ“Š",
            "unknown": "â“"
        }[metrics.trend_direction]
        
        lines.append(f"ğŸ† Overall Health Score: {metrics.overall_score:.1f}/100 (Grade: {metrics.grade})")
        if metrics.trend_direction != "unknown":
            lines.append(f"{trend_emoji} Improvement Trend: {metrics.trend_direction.title()}")
            if metrics.trend_percentage > 0:
                sign = "+" if metrics.trend_direction == "improving" else "-"
                lines.append(f"   {sign}{metrics.trend_percentage:.1f}% from last run")
        lines.append("")
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì§„í–‰ë¥  ë°”
        lines.append("Category Scores:")
        lines.append("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
        
        category_names = {
            'stateless_service': 'StatelessService',
            'service_method_decorator': 'Service Methods',
            'cache_decorator': 'Caching',
            'performance_decorator': 'Performance',
            'validation_decorator': 'Validation',
            'logging_decorator': 'Logging',
            'retry_decorator': 'Retry Logic',
            'railway_pattern': 'Error Handling',
            'result_type': 'Result Types'
        }
        
        for score in metrics.category_scores:
            name = category_names.get(score.name, score.name)
            # ì§„í–‰ë¥  ë°” ìƒì„± (10ë‹¨ê³„)
            progress = int((score.percentage / 100) * 10)
            bar = "â–ˆ" * progress + "â–‘" * (10 - progress)
            
            # í˜„ì¬/ì´ ê³„ì‚° (ê¸°íšŒê°€ ì ì„ìˆ˜ë¡ ì¢‹ìŒ)
            completed = score.total - score.current
            
            lines.append(f"â”‚ {name:<16} [{bar}] {score.percentage:.0f}% "
                        f"({completed}/{score.total}){'':>8}â”‚")
        
        lines.append("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
        lines.append("")
        
        # ì„±ìˆ™ë„ ë° ì˜ˆìƒ ì‹œê°„
        maturity_stars = "â˜…" * metrics.maturity_level + "â˜†" * (5 - metrics.maturity_level)
        lines.append(f"ğŸ“Š Maturity Level: {metrics.maturity_level}/5 ({maturity_stars})")
        lines.append(f"â±ï¸ Estimated Full Adoption: {metrics.estimated_hours:.0f} hours "
                    f"({metrics.total_batches} batches Ã— {metrics.estimated_hours/metrics.total_batches:.1f}h/batch)")
        lines.append("")
        
        # ìš°ì„ ìˆœìœ„ ê¶Œì¥ì‚¬í•­
        lines.append("ğŸ¯ Priority Recommendations:")
        lines.append("-" * 40)
        
        high_priority = [result for result in self.results 
                        for opp in result.opportunities if opp.severity == 'high']
        
        if high_priority:
            lines.append(f"ğŸ”´ HIGH: {len(high_priority)} critical improvements needed")
            lines.append("   Focus on StatelessService extensions first")
        
        medium_priority = self.statistics.get('severity_medium', 0)
        if medium_priority > 0:
            lines.append(f"ğŸŸ¡ MEDIUM: {medium_priority} performance optimizations available")
            lines.append("   Consider caching and performance decorators")
            
        low_priority = self.statistics.get('severity_low', 0)
        if low_priority > 0:
            lines.append(f"ğŸŸ¢ LOW: {low_priority} code quality improvements possible")
            lines.append("   Railway Pattern and Result types when time permits")
        
        lines.append("")
        lines.append("ğŸ’¡ Run with --show-batches to see implementation plan")
        lines.append("ğŸ’¡ Run with --apply-batch 1 to start with highest priority items")
        lines.append("â•" * 80)
        
        return "\n".join(lines)
    
    def _generate_batch_report(self) -> str:
        """ë°°ì¹˜ ë³´ê³ ì„œ ìƒì„±"""
        all_opportunities = self._get_all_opportunities()
        
        if not all_opportunities:
            return "No opportunities found to create batches."
        
        batch_applicator = BatchApplicator(all_opportunities, batch_size=10)
        batches = batch_applicator.create_batches()
        
        return batch_applicator.generate_batch_summary()
            
    def _generate_text_report(self) -> str:
        """í…ìŠ¤íŠ¸ ë³´ê³ ì„œ ìƒì„±"""
        lines = []
        lines.append("=" * 80)
        lines.append("RFS Framework Adoption Report")
        lines.append("=" * 80)
        lines.append("")
        
        # ìš”ì•½
        lines.append("ğŸ“Š Summary")
        lines.append("-" * 40)
        lines.append(f"Total files analyzed: {self.statistics['total_files']}")
        lines.append(f"Total opportunities: {self.statistics['total_opportunities']}")
        lines.append(f"Average RFS score: {self.statistics['average_score']:.1f}/100")
        lines.append("")
        
        # ì‹¬ê°ë„ë³„ ë¶„í¬
        lines.append("ğŸ¯ Severity Distribution")
        lines.append("-" * 40)
        for severity in ['high', 'medium', 'low']:
            count = self.statistics.get(f'severity_{severity}', 0)
            lines.append(f"  {severity.upper()}: {count}")
        lines.append("")
        
        # ê¸°íšŒ ìœ í˜•ë³„ ë¶„í¬
        lines.append("ğŸ“ˆ Opportunity Types")
        lines.append("-" * 40)
        opportunity_types = [
            ('stateless_service', 'StatelessService Extension'),
            ('service_method_decorator', '@service_method Decorator'),
            ('cache_decorator', '@cache_result Decorator'),
            ('retry_decorator', '@retry Decorator'),
            ('validation_decorator', '@validate_args Decorator'),
            ('logging_decorator', '@log_execution Decorator'),
            ('performance_decorator', '@measure_performance Decorator'),
            ('railway_pattern', 'Railway Pattern'),
            ('result_type', 'Result Type Usage')
        ]
        
        for opp_type, description in opportunity_types:
            count = self.statistics.get(opp_type, 0)
            if count > 0:
                lines.append(f"  {description}: {count}")
        lines.append("")
        
        # íŒŒì¼ë³„ ìƒì„¸
        if self.results:
            lines.append("ğŸ“ File Details")
            lines.append("-" * 40)
            
            # ì ìˆ˜ê°€ ë‚®ì€ ìˆœìœ¼ë¡œ ì •ë ¬
            sorted_results = sorted(self.results, key=lambda r: r.rfs_score)
            
            for result in sorted_results[:10]:  # ìƒìœ„ 10ê°œë§Œ
                try:
                    rel_path = Path(result.file_path).relative_to(Path.cwd())
                except ValueError:
                    rel_path = result.file_path
                lines.append(f"\n  {rel_path} (Score: {result.rfs_score:.0f}/100)")
                
                # ì‹¬ê°ë„ë³„ë¡œ ê·¸ë£¹í™”
                high_opps = [o for o in result.opportunities if o.severity == 'high']
                medium_opps = [o for o in result.opportunities if o.severity == 'medium']
                low_opps = [o for o in result.opportunities if o.severity == 'low']
                
                if high_opps:
                    lines.append("    ğŸ”´ High Priority:")
                    for opp in high_opps[:3]:
                        lines.append(f"      L{opp.line_number}: {opp.description}")
                        
                if medium_opps:
                    lines.append("    ğŸŸ¡ Medium Priority:")
                    for opp in medium_opps[:3]:
                        lines.append(f"      L{opp.line_number}: {opp.description}")
                        
                if low_opps and len(high_opps) + len(medium_opps) < 5:
                    lines.append("    ğŸŸ¢ Low Priority:")
                    for opp in low_opps[:2]:
                        lines.append(f"      L{opp.line_number}: {opp.description}")
                        
            if len(self.results) > 10:
                lines.append(f"\n  ... and {len(self.results) - 10} more files")
                
        lines.append("")
        lines.append("=" * 80)
        
        return "\n".join(lines)
        
    def _generate_json_report(self) -> str:
        """JSON ë³´ê³ ì„œ ìƒì„±"""
        report = {
            'summary': {
                'total_files': self.statistics['total_files'],
                'total_opportunities': self.statistics['total_opportunities'],
                'average_score': self.statistics['average_score']
            },
            'statistics': dict(self.statistics),
            'files': []
        }
        
        for result in self.results:
            file_info = {
                'path': result.file_path,
                'score': result.rfs_score,
                'opportunities': [opp.to_dict() for opp in result.opportunities]
            }
            report['files'].append(file_info)
            
        return json.dumps(report, indent=2, ensure_ascii=False)
        
    def _generate_markdown_report(self) -> str:
        """Markdown ë³´ê³ ì„œ ìƒì„±"""
        lines = []
        lines.append("# RFS Framework Adoption Report")
        lines.append("")
        
        lines.append("## ğŸ“Š Summary")
        lines.append("")
        lines.append(f"- **Files Analyzed**: {self.statistics['total_files']}")
        lines.append(f"- **Total Opportunities**: {self.statistics['total_opportunities']}")
        lines.append(f"- **Average Score**: {self.statistics['average_score']:.1f}/100")
        lines.append("")
        
        lines.append("## ğŸ¯ Recommendations")
        lines.append("")
        
        # ë†’ì€ ìš°ì„ ìˆœìœ„ ì¶”ì²œ
        high_priority = []
        for result in self.results:
            for opp in result.opportunities:
                if opp.severity == 'high':
                    high_priority.append((result, opp))
                    
        if high_priority:
            lines.append("### High Priority")
            lines.append("")
            for result, opp in high_priority[:5]:
                try:
                    rel_path = Path(result.file_path).relative_to(Path.cwd())
                except ValueError:
                    rel_path = result.file_path
                lines.append(f"- **{rel_path}:{opp.line_number}**")
                lines.append(f"  - Issue: {opp.description}")
                lines.append(f"  - Solution: {opp.suggestion}")
                lines.append("")
                
        return "\n".join(lines)
        
    def apply_fixes(self, auto_fix: bool = False) -> None:
        """ìë™ ìˆ˜ì • ì ìš© (í–¥í›„ êµ¬í˜„)"""
        if not auto_fix:
            print("Auto-fix is not implemented yet. Please apply suggestions manually.")
            return
            
        # TODO: ìë™ ìˆ˜ì • êµ¬í˜„
        # - StatelessService ìƒì† ì¶”ê°€
        # - ë°ì½”ë ˆì´í„° ìë™ ì¶”ê°€
        # - import ë¬¸ ì¶”ê°€
        pass


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description='Check RFS framework adoption in V2 codebase'
    )
    parser.add_argument(
        '--target-dir',
        default='app/services/v2',
        help='Target directory to analyze'
    )
    parser.add_argument(
        '--format',
        choices=['text', 'json', 'markdown'],
        default='text',
        help='Output format'
    )
    parser.add_argument(
        '--output',
        help='Output file (default: stdout)'
    )
    
    # ìŠ¤ì½”ì–´ë§ ë° ëŒ€ì‹œë³´ë“œ ì˜µì…˜
    parser.add_argument(
        '--dashboard',
        action='store_true',
        help='Show dashboard-style comprehensive scoring report'
    )
    parser.add_argument(
        '--baseline',
        action='store_true',
        help='Save current results as baseline for trend analysis'
    )
    
    # ë°°ì¹˜ ê´€ë¦¬ ì˜µì…˜
    parser.add_argument(
        '--show-batches',
        action='store_true',
        help='Show batch application plan (10 items per batch)'
    )
    parser.add_argument(
        '--batch-size',
        type=int,
        default=10,
        help='Number of items per batch (default: 10)'
    )
    parser.add_argument(
        '--apply-batch',
        type=int,
        metavar='N',
        help='Apply specific batch number (not implemented yet)'
    )
    parser.add_argument(
        '--priority',
        choices=['high', 'medium', 'low'],
        help='Filter opportunities by priority level'
    )
    
    # ê¸°ì¡´ ì˜µì…˜ë“¤
    parser.add_argument(
        '--fix',
        action='store_true',
        help='Apply automatic fixes (not implemented yet)'
    )
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='Verbose output'
    )
    
    args = parser.parse_args()
    
    # ê²€ì‚¬ ì‹¤í–‰
    checker = RFSAdoptionChecker(args.target_dir)
    checker.analyze()
    
    # ìš°ì„ ìˆœìœ„ í•„í„°ë§
    if args.priority:
        filtered_results = []
        for result in checker.results:
            filtered_opportunities = [
                opp for opp in result.opportunities 
                if opp.severity == args.priority
            ]
            if filtered_opportunities:
                filtered_result = FileAnalysisResult(
                    file_path=result.file_path,
                    opportunities=filtered_opportunities,
                    rfs_score=result.rfs_score
                )
                filtered_results.append(filtered_result)
        checker.results = filtered_results
        checker._calculate_statistics()
    
    # ê¸°ì¤€ì„  ì €ì¥
    if args.baseline:
        calculator = RFSScoreCalculator(checker.results, checker.statistics)
        metrics = calculator.calculate_comprehensive_score()
        calculator.save_baseline(metrics)
        print(f"âœ… Baseline saved for trend analysis")
    
    # ë³´ê³ ì„œ ìƒì„±
    report = checker.generate_report(
        format=args.format,
        dashboard=args.dashboard,
        show_batches=args.show_batches
    )
    
    # ì¶œë ¥
    if args.output:
        with open(args.output, 'w', encoding='utf-8') as f:
            f.write(report)
        print(f"âœ… Report saved to {args.output}")
    else:
        print(report)
    
    # ë°°ì¹˜ ì ìš© (í–¥í›„ êµ¬í˜„)
    if args.apply_batch:
        print(f"âš ï¸ Batch application not implemented yet. Would apply batch #{args.apply_batch}")
        print(f"ğŸ’¡ Use scripts/quality/rfs_apply_batch.py when available")
        
    # ìë™ ìˆ˜ì •
    if args.fix:
        checker.apply_fixes(auto_fix=True)
        
    # ì¢…ë£Œ ì½”ë“œ
    if checker.statistics['total_opportunities'] > 0:
        if checker.statistics.get('severity_high', 0) > 0:
            sys.exit(2)  # High severity issues found
        else:
            sys.exit(1)  # Medium/Low severity issues found
    else:
        sys.exit(0)  # No issues found


if __name__ == '__main__':
    main()